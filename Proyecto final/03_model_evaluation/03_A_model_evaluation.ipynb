{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluacion y analisis de modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "reg_dir = Path(\"../02_lineal_regression_results/regression_lineal\")\n",
        "prep_dir = Path(\"../01_preprocessing_results/preprocessing\")\n",
        "\n",
        "objetivo = \"Total_Amount\"\n",
        "train = pd.read_csv(prep_dir / \"T_train_final_objetivo.csv\")\n",
        "test = pd.read_csv(prep_dir / \"T_test_final_objetivo.csv\")\n",
        "\n",
        "X_train = train.drop(columns=[objetivo])\n",
        "y_train = train[objetivo]\n",
        "X_test = test.drop(columns=[objetivo])\n",
        "y_test = test[objetivo]\n",
        "\n",
        "lin = joblib.load(reg_dir / \"modelo_reg_lineal.pkl\")\n",
        "rf = joblib.load(reg_dir / \"modelo_random_forest.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Evaluar los modelos cargados\n",
        "\n",
        "def evaluar(nombre, modelo):\n",
        "    preds = modelo.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, preds)\n",
        "    rmse = float(np.sqrt(mse))\n",
        "    mae = float(mean_absolute_error(y_test, preds))\n",
        "    r2 = float(r2_score(y_test, preds))\n",
        "    return {\"modelo\": nombre, \"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"preds\": preds}\n",
        "\n",
        "res_lin = evaluar(\"LinearRegression\", lin)\n",
        "res_rf = evaluar(\"RandomForestRegressor\", rf)\n",
        "\n",
        "resultados = pd.DataFrame([\n",
        "    {k: v for k, v in res_lin.items() if k != \"preds\"},\n",
        "    {k: v for k, v in res_rf.items() if k != \"preds\"},\n",
        "])\n",
        "resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Importancias de variables para el RandomForest (modelo ganador)\n",
        "importancias = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
        "importancias_10 = importancias.sort_values(ascending=False).head(10)\n",
        "importancias_df = importancias.sort_values(ascending=False).reset_index()\n",
        "importancias_df.columns = [\"feature\", \"importance\"]\n",
        "importancias_df.to_csv(reg_dir / \"importancias_random_forest.csv\", index=False)\n",
        "importancias_10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Resumen y guardado\n",
        "resultados.to_csv(reg_dir / \"resumen_evaluacion.csv\", index=False)\n",
        "mejor = max([res_lin, res_rf], key=lambda r: r[\"r2\"])\n",
        "print(\"Mejor modelo:\", mejor[\"modelo\"], \"R2=\", mejor[\"r2\"])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}